import os
import torch
import glob
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm

# ================= é…ç½® =================
MODEL_PATH = "/mnt/afs/250010074/qwen/Qwen3-4B-Base"
CACHE_ROOT = "/mnt/afs/250010074/qwen/benchmark_cache/datasets"

# è‡ªåŠ¨æŸ¥æ‰¾ Arrow æ–‡ä»¶çš„å‡½æ•°
def find_arrow_file(dataset_name, split_name):
    # æœç´¢æ¨¡å¼ï¼šdatasets/namespace___dataset_name/**/dataset_name-split.arrow
    # ä¾‹å¦‚: datasets/baber___piqa/**/piqa-validation.arrow
    pattern = os.path.join(CACHE_ROOT, f"*{dataset_name}*", "**", f"*{split_name}.arrow")
    files = glob.glob(pattern, recursive=True)
    if files:
        return files[0] # è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ª
    return None

# æ•°æ®é›†é…ç½®
# key: ä»»åŠ¡å
# name: æ•°æ®é›†æ–‡ä»¶åéƒ¨åˆ† (å¦‚ piqa-validation.arrow ä¸­çš„ piqa)
# split: split åç§°
DATASET_CONFIGS = {
    "piqa": {"name": "piqa", "split": "validation"},  # test é›†æ²¡æœ‰å…¬å¼€ç­”æ¡ˆï¼Œåªæœ‰ validation é›†å¯ç”¨äºæœ¬åœ°è¯„æµ‹å’Œæ¨¡å‹å¼€å‘
    "arc": {"name": "ai2_arc", "split": "test"}, # ARC-Challenge é€šå¸¸ç”¨ test é›†
    "winogrande": {"name": "winogrande", "split": "validation"} # test é›†æ²¡æœ‰å…¬å¼€ç­”æ¡ˆï¼Œåªæœ‰ validation é›†å¯ç”¨äºæœ¬åœ°è¯„æµ‹å’Œæ¨¡å‹å¼€å‘
}

def load_local_dataset(task_name):
    config = DATASET_CONFIGS[task_name]
    arrow_file = find_arrow_file(config["name"], config["split"])
    
    if not arrow_file:
        print(f"âŒ æœªæ‰¾åˆ° {task_name} çš„æœ¬åœ° Arrow æ–‡ä»¶ã€‚")
        return None
    
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ–‡ä»¶: {arrow_file}")
    # ä½¿ç”¨ arrow æ ¼å¼ç›´æ¥åŠ è½½
    ds = load_dataset("arrow", data_files={config["split"]: arrow_file}, split=config["split"])
    return ds

def evaluate_piqa(model, tokenizer, dataset):
    correct = 0
    total = 0
    
    print("ğŸ”„ æ­£åœ¨è¯„ä¼° PIQA...")
    for example in tqdm(dataset):
        goal = example['goal']
        sol1 = example['sol1']
        sol2 = example['sol2']
        label = example['label'] # 0 or 1
        
        # æ„å»º prompt (Zero-shot æ ¼å¼)
        prompt = f"Question: {goal}\nAnswer:"
        
        # ç®€å•çš„ likelihood æ¯”è¾ƒ
        # è®¡ç®— sol1 å’Œ sol2 çš„ perplexity æˆ–è€…ç›´æ¥ç”Ÿæˆ
        # è¿™é‡Œä½¿ç”¨ç”Ÿæˆæ³•æ¯”è¾ƒç®€å•ï¼Œä½† likelihood æ›´å‡†ç¡®ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œæ²¿ç”¨ç”Ÿæˆæ³•æˆ–ç®€å•çš„åŒ…å«åˆ¤æ–­
        # ä½†ä¸ºäº†æ›´æ¥è¿‘ lm-evalï¼Œæˆ‘ä»¬åº”è¯¥æ¯”è¾ƒ log-likelihoodã€‚
        # è¿™é‡Œä¸ºäº†ä»£ç ç®€æ´ï¼Œä½¿ç”¨ç”Ÿæˆæ³• + é€‰é¡¹åŒ¹é… (ç±»ä¼¼äº test.py çš„é€»è¾‘ï¼Œä½†ç¨ä½œæ”¹è¿›)
        
        # æ”¹è¿›ï¼šä½¿ç”¨ log-likelihood æ¯”è¾ƒ (æ›´æ ‡å‡†)
        # æ„é€ ä¸¤ä¸ªå®Œæ•´çš„å¥å­
        ctx = f"Question: {goal}\nAnswer:"
        cand1 = f" {sol1}"
        cand2 = f" {sol2}"
        
        score1 = get_log_prob(model, tokenizer, ctx, cand1)
        score2 = get_log_prob(model, tokenizer, ctx, cand2)
        
        pred = 0 if score1 > score2 else 1
        
        if pred == label:
            correct += 1
        total += 1
        
    return correct / total

def evaluate_arc(model, tokenizer, dataset):
    correct = 0
    total = 0
    
    print("ğŸ”„ æ­£åœ¨è¯„ä¼° ARC-Challenge...")
    for example in tqdm(dataset):
        question = example['question']
        choices = example['choices'] # {'text': [...], 'label': [...]}
        answerKey = example['answerKey']
        
        scores = []
        labels = choices['label']
        texts = choices['text']
        
        ctx = f"Question: {question}\nAnswer:"
        
        for text in texts:
            cand = f" {text}"
            score = get_log_prob(model, tokenizer, ctx, cand)
            scores.append(score)
            
        # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„ç´¢å¼•
        best_idx = scores.index(max(scores))
        pred_label = labels[best_idx]
        
        if pred_label == answerKey:
            correct += 1
        total += 1
        
    return correct / total

def evaluate_winogrande(model, tokenizer, dataset):
    correct = 0
    total = 0
    
    print("ğŸ”„ æ­£åœ¨è¯„ä¼° Winogrande...")
    for example in tqdm(dataset):
        sentence = example['sentence']
        option1 = example['option1']
        option2 = example['option2']
        label = int(example['answer']) # "1" or "2" -> 1 or 2
        
        # Winogrande éœ€è¦æ›¿æ¢ _ ä¸ºé€‰é¡¹
        if "_" not in sentence:
            # æŸäº›æ ·æœ¬å¯èƒ½æ²¡æœ‰ _ï¼Œä½œä¸º fallback
            ctx = sentence + " "
        else:
            ctx = sentence.split("_")[0] # å– _ å‰é¢çš„éƒ¨åˆ†ä½œä¸º context (ç®€åŒ–ç‰ˆ)
            # æ›´æ ‡å‡†çš„åšæ³•æ˜¯æ›¿æ¢ _ å¹¶è®¡ç®—æ•´ä¸ªå¥å­çš„ perplexity
        
        # ç®€å•åšæ³•ï¼šæ›¿æ¢ _
        sent1 = sentence.replace("_", option1)
        sent2 = sentence.replace("_", option2)
        
        # è®¡ç®—æ•´ä¸ªå¥å­çš„ log-prob
        score1 = get_sentence_log_prob(model, tokenizer, sent1)
        score2 = get_sentence_log_prob(model, tokenizer, sent2)
        
        pred = 1 if score1 > score2 else 2
        
        if pred == label:
            correct += 1
        total += 1
        
    return correct / total

def get_log_prob(model, tokenizer, context, candidate):
    # è®¡ç®— P(candidate | context)
    input_text = context + candidate
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    # æ‰¾åˆ° candidate çš„ token èŒƒå›´
    ctx_len = len(tokenizer(context, add_special_tokens=False)['input_ids'])
    # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ tokenizer ä¸ä¼šå› ä¸ºæ‹¼æ¥è€Œæ”¹å˜ tokenization
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        
    # Shift logits
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = inputs.input_ids[..., 1:].contiguous()
    
    # åªè®¡ç®— candidate éƒ¨åˆ†çš„ loss
    # candidate å¯¹åº”çš„ labels æ˜¯ shift_labels[ctx_len-1:] (å¤§çº¦)
    # ä¸ºäº†å‡†ç¡®ï¼Œæˆ‘ä»¬è®¡ç®—æ•´ä¸ªåºåˆ—çš„ lossï¼Œç„¶åå‡å» context çš„ lossï¼Œæˆ–è€…åªå–ååŠéƒ¨åˆ†
    # è¿™é‡Œä½¿ç”¨ç®€å•çš„ gather æ–¹æ³•
    
    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
    
    # Gather log probs of the correct tokens
    target_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)
    
    # Sum log probs for the candidate tokens
    # å‡è®¾ candidate ä» ctx_len å¼€å§‹ (è¿™å–å†³äº tokenizer æ˜¯å¦æ·»åŠ  BOS)
    # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å®ç°
    candidate_log_prob = target_log_probs[0, ctx_len:].sum().item()
    
    return candidate_log_prob

def get_sentence_log_prob(model, tokenizer, sentence):
    inputs = tokenizer(sentence, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs.input_ids)
        # loss æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶
        loss = outputs.loss
        # log_prob = -loss * seq_len
        return -loss.item() * inputs.input_ids.size(1)

def main():
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype=torch.float16)
    
    results = {}
    
    # PIQA
    ds_piqa = load_local_dataset("piqa")
    if ds_piqa:
        acc = evaluate_piqa(model, tokenizer, ds_piqa)
        results["PIQA"] = acc
        print(f"âœ… PIQA Accuracy: {acc:.2%}")
        
    # ARC
    ds_arc = load_local_dataset("arc")
    if ds_arc:
        acc = evaluate_arc(model, tokenizer, ds_arc)
        results["ARC-Challenge"] = acc
        print(f"âœ… ARC-Challenge Accuracy: {acc:.2%}")
        
    # Winogrande
    ds_wino = load_local_dataset("winogrande")
    if ds_wino:
        acc = evaluate_winogrande(model, tokenizer, ds_wino)
        results["Winogrande"] = acc
        print(f"âœ… Winogrande Accuracy: {acc:.2%}")
        
    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    for k, v in results.items():
        print(f"{k}: {v:.2%}")

if __name__ == "__main__":
    main()
