import os
import torch
import glob
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset
from tqdm import tqdm

# ================= é…ç½® =================
# 1. Base æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "/mnt/afs/250010074/qwen/Qwen3-4B-Base"

# 2. SFT æ¨¡å‹è·¯å¾„ (å…¨é‡æˆ– LoRA)
# å¦‚æœ SFT æ˜¯å…¨é‡å¾®è°ƒï¼Œç›´æ¥å†™è·¯å¾„ï¼›å¦‚æœæ˜¯ LoRAï¼Œå†™ Adapter è·¯å¾„
# è¿™é‡Œå‡è®¾ SFT æ˜¯ LoRAï¼Œä¸”å·²ç» merge æˆäº† final_model (æˆ–è€…ç›´æ¥åŠ è½½ final_model)
# æ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼ŒSFT çš„ final_model æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ç›®å½•
SFT_MODEL_PATH = "/mnt/afs/250010074/qwen/output/sft_arc_checkpoints_2025-12-23_17-42-41/final_model"

# 3. RL (DPO) æ¨¡å‹è·¯å¾„
# DPO æ˜¯åŸºäº SFT è®­ç»ƒçš„ LoRA Adapter
DPO_ADAPTER_PATH = "/mnt/afs/250010074/qwen/rl_output/dpo_arc_2025-12-23_19-28-42/final_model"

CACHE_ROOT = "/mnt/afs/250010074/qwen/benchmark_cache/datasets"

# è‡ªåŠ¨æŸ¥æ‰¾ Arrow æ–‡ä»¶çš„å‡½æ•°
def find_arrow_file(dataset_name, split_name):
    pattern = os.path.join(CACHE_ROOT, f"*{dataset_name}*", "**", f"*{split_name}.arrow")
    files = glob.glob(pattern, recursive=True)
    if files:
        return files[0]
    return None

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    "arc": {"name": "ai2_arc", "split": "test"}, 
}

def load_local_dataset(task_name):
    config = DATASET_CONFIGS[task_name]
    arrow_file = find_arrow_file(config["name"], config["split"])
    
    if not arrow_file:
        print(f"âŒ æœªæ‰¾åˆ° {task_name} çš„æœ¬åœ° Arrow æ–‡ä»¶ã€‚")
        return None
    
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ–‡ä»¶: {arrow_file}")
    ds = load_dataset("arrow", data_files={config["split"]: arrow_file}, split=config["split"])
    return ds

def get_log_prob(model, tokenizer, context, candidate):
    input_text = context + candidate
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    ctx_len = len(tokenizer(context, add_special_tokens=False)['input_ids'])
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = inputs.input_ids[..., 1:].contiguous()
    
    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
    target_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)
    
    # ç®€å•çš„ mask å¤„ç†ï¼Œåªè®¡ç®— candidate éƒ¨åˆ†
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ tokenizer è¡Œä¸ºä¸€è‡´ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´ä¸¥è°¨çš„å¤„ç†
    if ctx_len < target_log_probs.size(1):
        candidate_log_prob = target_log_probs[0, ctx_len:].sum().item()
    else:
        candidate_log_prob = -9999.0 # å¼‚å¸¸æƒ…å†µ
    
    return candidate_log_prob

def evaluate_arc(model, tokenizer, dataset):
    correct = 0
    total = 0
    
    print("ğŸ”„ æ­£åœ¨è¯„ä¼° ARC-Challenge...")
    for example in tqdm(dataset):
        question = example['question']
        choices = example['choices']
        answerKey = example['answerKey']
        
        scores = []
        labels = choices['label']
        texts = choices['text']
        
        # æ„é€  Promptï¼Œä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´çš„é£æ ¼ (è™½ç„¶è¿™é‡Œæ˜¯è¯„æµ‹ï¼Œä½†å°½é‡ä¿æŒä¸€è‡´)
        # è®­ç»ƒæ—¶: User: {question}\nChoices:\n{options}\nAnswer:\nAssistant: {answer}
        # è¯„æµ‹æ—¶: ç»™å®š Contextï¼Œçœ‹å“ªä¸ª Option çš„æ¦‚ç‡å¤§
        # è¿™é‡Œä½¿ç”¨æ ‡å‡†çš„ Zero-shot æ ¼å¼: Question: ... Answer: ...
        
        ctx = f"Question: {question}\nAnswer:"
        
        for text in texts:
            cand = f" {text}"
            score = get_log_prob(model, tokenizer, ctx, cand)
            scores.append(score)
            
        best_idx = scores.index(max(scores))
        pred_label = labels[best_idx]
        
        if pred_label == answerKey:
            correct += 1
        total += 1
        
    return correct / total

def main():
    results = {}
    ds_arc = load_local_dataset("arc")
    if not ds_arc:
        return

    # 1. è¯„ä¼° Base æ¨¡å‹
    print(f"\nğŸš€ [1/3] Evaluating Base Model: {BASE_MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH, 
        trust_remote_code=True, 
        device_map="auto", 
        torch_dtype=torch.float16
    )
    model.eval()
    acc_base = evaluate_arc(model, tokenizer, ds_arc)
    results["Base"] = acc_base
    print(f"âœ… Base Accuracy: {acc_base:.2%}")
    
    # é‡Šæ”¾æ˜¾å­˜
    del model
    torch.cuda.empty_cache()

    # 2. è¯„ä¼° SFT æ¨¡å‹
    print(f"\nğŸš€ [2/3] Evaluating SFT Model: {SFT_MODEL_PATH}")
    # æ³¨æ„ï¼šå¦‚æœ SFT æ˜¯ LoRA Adapterï¼Œéœ€è¦å…ˆåŠ è½½ Base å†åŠ è½½ Adapterã€‚
    # è¿™é‡Œå‡è®¾ SFT_MODEL_PATH æ˜¯å·²ç» merge å¥½çš„å®Œæ•´æ¨¡å‹ (æ ¹æ®ä¹‹å‰çš„ final_model é€»è¾‘)
    model = AutoModelForCausalLM.from_pretrained(
        SFT_MODEL_PATH, 
        trust_remote_code=True, 
        device_map="auto", 
        torch_dtype=torch.float16
    )
    model.eval()
    acc_sft = evaluate_arc(model, tokenizer, ds_arc)
    results["SFT"] = acc_sft
    print(f"âœ… SFT Accuracy: {acc_sft:.2%}")

    # 3. è¯„ä¼° RL (DPO) æ¨¡å‹
    print(f"\nğŸš€ [3/3] Evaluating RL (DPO) Model")
    print(f"Base for RL: {SFT_MODEL_PATH}")
    print(f"Adapter for RL: {DPO_ADAPTER_PATH}")
    
    # DPO æ˜¯åŸºäº SFT æ¨¡å‹çš„ LoRAï¼Œæ‰€ä»¥åŸºåº§æ˜¯ SFT æ¨¡å‹
    # æˆ‘ä»¬å¤ç”¨åˆšæ‰åŠ è½½çš„ SFT modelï¼Œç›´æ¥åŠ è½½ Adapter
    # å¦‚æœåˆšæ‰é‡Šæ”¾äº†ï¼Œéœ€è¦é‡æ–°åŠ è½½ SFT æ¨¡å‹
    # model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH, ...) 
    
    model = PeftModel.from_pretrained(model, DPO_ADAPTER_PATH)
    print("ğŸ”„ Merging LoRA weights...")
    model = model.merge_and_unload()
    model.eval()
    
    acc_rl = evaluate_arc(model, tokenizer, ds_arc)
    results["RL (DPO)"] = acc_rl
    print(f"âœ… RL (DPO) Accuracy: {acc_rl:.2%}")
        
    print("\nğŸ“Š æœ€ç»ˆå¯¹æ¯”ç»“æœ:")
    print(f"{'Stage':<15} | {'Accuracy':<10}")
    print("-" * 30)
    for k, v in results.items():
        print(f"{k:<15} | {v:.2%}")

if __name__ == "__main__":
    main()
