import os
import torch
import glob
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset
from tqdm import tqdm

# ================= é…ç½® =================
# 1. Base æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "/mnt/afs/250010074/qwen/Qwen3-4B-Base"

# 2. SFT æ¨¡å‹è·¯å¾„ (å…¨é‡æˆ– LoRA)
# å‡è®¾ SFT æ˜¯ LoRAï¼Œä¸”å·²ç» merge æˆäº† final_model
SFT_MODEL_PATH = "/mnt/afs/250010074/qwen/output/sft_piqa_checkpoints_xxxx/final_model" # éœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„

# 3. RL (DPO) æ¨¡å‹è·¯å¾„
# DPO æ˜¯åŸºäº SFT è®­ç»ƒçš„ LoRA Adapter
DPO_ADAPTER_PATH = "/mnt/afs/250010074/qwen/rl_output/dpo_piqa_xxxx/final_model" # éœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„

CACHE_ROOT = "/mnt/afs/250010074/qwen/benchmark_cache/datasets"

def find_arrow_file(dataset_name, split_name):
    pattern = os.path.join(CACHE_ROOT, f"*{dataset_name}*", "**", f"*{split_name}.arrow")
    files = glob.glob(pattern, recursive=True)
    if files:
        return files[0]
    return None

DATASET_CONFIGS = {
    "piqa": {"name": "piqa", "split": "validation"}, 
}

def load_local_dataset(task_name):
    config = DATASET_CONFIGS[task_name]
    arrow_file = find_arrow_file(config["name"], config["split"])
    if not arrow_file:
        print(f"âŒ æœªæ‰¾åˆ° {task_name} çš„æœ¬åœ° Arrow æ–‡ä»¶ã€‚")
        return None
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ–‡ä»¶: {arrow_file}")
    ds = load_dataset("arrow", data_files={config["split"]: arrow_file}, split=config["split"])
    return ds

def get_log_prob(model, tokenizer, context, candidate):
    input_text = context + candidate
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    ctx_len = len(tokenizer(context, add_special_tokens=False)['input_ids'])
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = inputs.input_ids[..., 1:].contiguous()
    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)
    target_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)
    if ctx_len < target_log_probs.size(1):
        candidate_log_prob = target_log_probs[0, ctx_len:].sum().item()
    else:
        candidate_log_prob = -9999.0
    return candidate_log_prob

def evaluate_piqa(model, tokenizer, dataset):
    correct = 0
    total = 0
    print("ğŸ”„ æ­£åœ¨è¯„ä¼° PIQA...")
    for example in tqdm(dataset):
        goal = example['goal']
        sol1 = example['sol1']
        sol2 = example['sol2']
        label = example['label']
        
        ctx = f"Question: {goal}\nAnswer:"
        cand1 = f" {sol1}"
        cand2 = f" {sol2}"
        
        score1 = get_log_prob(model, tokenizer, ctx, cand1)
        score2 = get_log_prob(model, tokenizer, ctx, cand2)
        
        pred = 0 if score1 > score2 else 1
        if pred == label:
            correct += 1
        total += 1
    return correct / total

def main():
    results = {}
    ds_piqa = load_local_dataset("piqa")
    if not ds_piqa:
        return

    # 1. Base
    print(f"\nğŸš€ [1/3] Evaluating Base Model: {BASE_MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype=torch.float16)
    model.eval()
    acc_base = evaluate_piqa(model, tokenizer, ds_piqa)
    results["Base"] = acc_base
    print(f"âœ… Base Accuracy: {acc_base:.2%}")
    del model
    torch.cuda.empty_cache()

    # 2. SFT
    print(f"\nğŸš€ [2/3] Evaluating SFT Model: {SFT_MODEL_PATH}")
    if os.path.exists(SFT_MODEL_PATH):
        model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype=torch.float16)
        model.eval()
        acc_sft = evaluate_piqa(model, tokenizer, ds_piqa)
        results["SFT"] = acc_sft
        print(f"âœ… SFT Accuracy: {acc_sft:.2%}")
        del model
        torch.cuda.empty_cache()
    else:
        print(f"âš ï¸ SFT Model path not found: {SFT_MODEL_PATH}")

    # 3. RL (DPO)
    print(f"\nğŸš€ [3/3] Evaluating RL (DPO) Model")
    if os.path.exists(SFT_MODEL_PATH) and os.path.exists(DPO_ADAPTER_PATH):
        model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype=torch.float16)
        model = PeftModel.from_pretrained(model, DPO_ADAPTER_PATH)
        print("ğŸ”„ Merging LoRA weights...")
        model = model.merge_and_unload()
        model.eval()
        acc_rl = evaluate_piqa(model, tokenizer, ds_piqa)
        results["RL (DPO)"] = acc_rl
        print(f"âœ… RL (DPO) Accuracy: {acc_rl:.2%}")
    else:
        print(f"âš ï¸ Model paths not found for RL evaluation.")

    print("\nğŸ“Š æœ€ç»ˆå¯¹æ¯”ç»“æœ:")
    print(f"{'Stage':<15} | {'Accuracy':<10}")
    print("-" * 30)
    for k, v in results.items():
        print(f"{k:<15} | {v:.2%}")

if __name__ == "__main__":
    main()
